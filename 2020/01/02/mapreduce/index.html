<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="Mogician zlf">
  <!-- Open Graph Data -->
  <meta property="og:title" content="MapReduce(Windows版)"/>
  <meta property="og:description" content="" />
  <meta property="og:site_name" content="Mogician的个人小站"/>
  <meta property="og:type" content="article" />
  <meta property="og:image" content="http://yoursite.com"/>
  
    <link rel="alternate" href="/atom.xml" title="Mogician的个人小站" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>Mogician的个人小站</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/default-banner-dark.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">MapReduce(Windows版)</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/<your-github-username>" target="_blank" rel="noopener">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:<your-email-address>" target="_blank" rel="noopener">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By Mogician zlf</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2020-01-02</span>
            <span class="time">19:49:26</span>
          </span>
          
        </div>
        <!-- Tags -->
        
        <!-- Post Main Content -->
        <div class="post-content">
          <h1 id="MapReduce练习"><a href="#MapReduce练习" class="headerlink" title="MapReduce练习"></a>MapReduce练习</h1><h2 id="Python版"><a href="#Python版" class="headerlink" title="Python版"></a>Python版</h2><h3 id="Udacity练习"><a href="#Udacity练习" class="headerlink" title="Udacity练习"></a>Udacity练习</h3><p>首先我完成了老师布置的Udacity上的mapreduce练习题</p>
<p><img src="https://raw.githubusercontent.com/mogician-alt/PicBed/dev/20191230160611.png" alt=""></p>
<p>python很简单，再给出的mapper和reducer两个py文件中补全代码即可。 </p>
<p>第一题求和，第二题求平均，第三题求最大值及对应时间，都需要注意跳过表头</p>
<p><img src="https://raw.githubusercontent.com/mogician-alt/PicBed/dev/20191230160834.png" alt=""></p>
<h2 id="表自然连接"><a href="#表自然连接" class="headerlink" title="表自然连接"></a>表自然连接</h2><p>student_course表：(SID, CID, SCORE, TID)</p>
<p>student表：(SID, NAME, SEX, AGE, BIRTHDAY, DNAME, CLASS)</p>
<p>均有表头</p>
<h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>两个表通过mapper分别转换为 &lt;SID, “S”, others…&gt;和 &lt;SID, “SC”, others&gt; 的形式，然后通过shuffle排好序，很容易可以得到多串相同SID的行，在reducer中，对于相同的SID，判断是“S”还是“SC”，然后笛卡尔积即可。</p>
<p>另一种替代方式是在reducer判断中，根据属性的数目，sc是4个，s是7个。但这样做并不普适。</p>
<p>查询知，streaming可以通过以下方式获得输入文件名</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    input_file = os.environ[<span class="string">'mapreduce_map_input_file'</span>]</span><br><span class="line"><span class="keyword">except</span> KeyError:</span><br><span class="line">    input_file = os.environ[<span class="string">'map_input_file'</span>]  <span class="comment"># 老版本</span></span><br></pre></td></tr></table></figure>

<h3 id="Hadoop-Streaming方式"><a href="#Hadoop-Streaming方式" class="headerlink" title="Hadoop Streaming方式"></a>Hadoop Streaming方式</h3><h4 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h4><p>这学期选修的另外一门非关系型数据库课程实验中，我已经在本地安装好了Hadoop</p>
<p><img src="https://raw.githubusercontent.com/mogician-alt/PicBed/dev/20191230162242.png" alt=""></p>
<p>mapper.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        input_file = os.environ[<span class="string">'mapreduce_map_input_file'</span>]</span><br><span class="line">    <span class="keyword">except</span> KeyError:</span><br><span class="line">        input_file = os.environ[<span class="string">'map_input_file'</span>]</span><br><span class="line">    input_file = os.path.basename(input_file).split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">    tag = <span class="string">'sc'</span> <span class="keyword">if</span> input_file == <span class="string">'student_course'</span> <span class="keyword">else</span> <span class="string">'s'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">        data = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        print(<span class="string">'\t'</span>.join([data[<span class="number">0</span>]] + [tag] + data[<span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">mapper()</span><br></pre></td></tr></table></figure>

<p>reducer.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    ss = []</span><br><span class="line">    scs = []</span><br><span class="line">    old_key = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">        l = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        key = l[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> old_key <span class="keyword">and</span> key != old_key:</span><br><span class="line">            <span class="keyword">if</span> ss <span class="keyword">and</span> scs:</span><br><span class="line">                print(<span class="string">'\n'</span>.join([<span class="string">'\t'</span>.join([key] + s + sc) <span class="keyword">for</span> sc <span class="keyword">in</span> scs <span class="keyword">for</span> s <span class="keyword">in</span> ss]))</span><br><span class="line">            ss = []</span><br><span class="line">            scs = []</span><br><span class="line"></span><br><span class="line">        old_key = key</span><br><span class="line">        <span class="keyword">if</span> l[<span class="number">1</span>] == <span class="string">'s'</span>:</span><br><span class="line">            ss.append(l[<span class="number">2</span>:])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            scs.append(l[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> old_key <span class="keyword">and</span> ss <span class="keyword">and</span> scs:</span><br><span class="line">        print(<span class="string">'\n'</span>.join([<span class="string">'\t'</span>.join([old_key] + s + sc) <span class="keyword">for</span> sc <span class="keyword">in</span> scs <span class="keyword">for</span> s <span class="keyword">in</span> ss]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">reducer()</span><br></pre></td></tr></table></figure>

<p> 配置一下环境变量（cmd里set是临时的大概，只在该窗口内有效）</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">C:\<span class="title">Users</span>\<span class="title">Stranded</span>&gt;<span class="title">set</span> <span class="title">HADOOP_HOME</span></span></span><br><span class="line"><span class="function"><span class="title">HADOOP_HOME</span>=<span class="title">D</span>:\<span class="title">Nosql</span>\<span class="title">Hadoop</span>\<span class="title">hadoop</span>-2.7.7</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">C</span>:\<span class="title">Users</span>\<span class="title">Stranded</span>&gt;<span class="title">set</span> <span class="title">STREAM</span> = %<span class="title">HADOOP_HOME</span>%\<span class="title">share</span>\<span class="title">hadoop</span>\<span class="title">tools</span>\<span class="title">lib</span>\<span class="title">hadoop</span>-<span class="title">streaming</span>-2.7.7.<span class="title">jar</span></span></span><br></pre></td></tr></table></figure>

<p>（linux参考 <a href="https://www.jianshu.com/p/6148cb62bdb0" target="_blank" rel="noopener">https://www.jianshu.com/p/6148cb62bdb0</a> ）</p>
<p>先看看写的对不对，结果还真写错了…</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">type</span> student.txt + student_course.txt | python mapper.py | sort | python reducer.py</span><br></pre></td></tr></table></figure>

<p>修改后，写一个bat脚本（注意，windows上用streaming和网上一般能查到的linux脚本是不一样的，不能使用-files传多个文件，必须-file一个一个上传，另外-mapper等也得是cmd或者jar的形式，可以通过<code>hadoop jar %STREAM% -help</code> 的方式查看）</p>
<p>参考 <a href="https://blog.csdn.net/weixin_33994444/article/details/87992731" target="_blank" rel="noopener">https://blog.csdn.net/weixin_33994444/article/details/87992731</a> </p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">::hdfs dfs -<span class="built_in">mkdir</span> /user</span><br><span class="line">::hdfs dfs -<span class="built_in">mkdir</span> /user/input</span><br><span class="line">::hdfs dfs -put ./*.txt /user/input</span><br><span class="line">hadoop jar <span class="variable">%STREAM%</span> ^</span><br><span class="line">        -D stream.non.zero.<span class="keyword">exit</span>.is.failure=false ^</span><br><span class="line">        -file mapper.py ^</span><br><span class="line">        -file reducer.py ^</span><br><span class="line">        -input /user/input ^</span><br><span class="line">        -output /user/output ^</span><br><span class="line">        -mapper "python mapper.py" ^</span><br><span class="line">        -reducer "python reducer.py"</span><br></pre></td></tr></table></figure>

<p>注意最开始要在hdfs里创建目录、上传输入文件</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) PS C:\Users\Stranded\PycharmProjects\Aliyun\hadoopTest&gt; hdfs dfs -ls /user/input</span><br><span class="line">Found <span class="number">2</span> items</span><br><span class="line">-rw-r--r--   <span class="number">1</span> Stranded supergroup     <span class="number">253605</span> <span class="number">2019</span>-<span class="number">12</span>-<span class="number">30</span> <span class="number">17</span>:<span class="number">54</span> /user/input/student.txt</span><br><span class="line">-rw-r--r--   <span class="number">1</span> Stranded supergroup    <span class="number">1013707</span> <span class="number">2019</span>-<span class="number">12</span>-<span class="number">30</span> <span class="number">17</span>:<span class="number">54</span> /user/input/student_course.txt</span><br></pre></td></tr></table></figure>

<p>报错</p>
<blockquote>
<p>There are 0 datanode(s) running and no node(s) are excluded in this operation</p>
</blockquote>
<p>解决：</p>
<blockquote>
<p> 将hdfs-site.xml配置文件中的 dfs.datanode.data.dir配置项对应的文件夹下的cureent文件夹删除 </p>
<p> start-all重启，jps里有datanode了</p>
</blockquote>
<p>报错</p>
<blockquote>
<p>Exception message: CreateSymbolicLink error (1314): ???????????</p>
</blockquote>
<p>解决：</p>
<blockquote>
<p> 修改了core-site.xml文件，要添加 fs.defaultFS 和 fs.default.name两个相同的属性 </p>
<p> <a href="https://blog.csdn.net/qq_29477175/article/details/89683491" target="_blank" rel="noopener">https://blog.csdn.net/qq_29477175/article/details/89683491</a> </p>
</blockquote>
<p>报错：</p>
<blockquote>
<p>safemode</p>
</blockquote>
<p>解决</p>
<blockquote>
<p> 等待他过几秒自动OFF或者</p>
<p> hdfs dfsadmin -safemode leave </p>
</blockquote>
<p>还有其他一堆报错，我太南了</p>
<p>各种排除问题，最后发现可能是bin的问题，之前下的是2.7.1的windows编译版本，2.7.4前后不一样，所以重新下了个2.7.7的</p>
<p> <a href="https://github.com/cdarlint/winutils/tree/master/hadoop-2.7.7" target="_blank" rel="noopener">https://github.com/cdarlint/winutils/tree/master/hadoop-2.7.7</a> </p>
<p>还要注意！要把bin文件里的hadoop.dll复制到C:/Windows/System32里，之前原本的没覆盖就还是有问题。</p>
<p>job运行成功后，查看各个统计信息，我最开始输出是0，修改脚本，去掉combiner就有输出了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">(base) PS C:\Users\Stranded\PycharmProjects\Aliyun\hadoopTest&gt; .\run.bat</span><br><span class="line"></span><br><span class="line">C:\Users\Stranded\PycharmProjects\Aliyun\hadoopTest&gt;hadoop jar D:\Nosql\Hadoop\hadoop-2.7.7\share\hadoop\tools\lib\hadoop-streaming-2.7.7.jar         -D stream.non.zero.exit.is.failure=false         -file mapper.py         -file reducer.py         -input /user/input         -output /user/output         -mapper &quot;python mapper.py&quot;         -reducer &quot;python reducer.py&quot;</span><br><span class="line">19/12/31 12:51:05 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.</span><br><span class="line">packageJobJar: [mapper.py, reducer.py, /C:/Users/Stranded/AppData/Local/Temp/hadoop-unjar4405973670165844345/] [] C:\Users\Stranded\AppData\Local\Temp\streamjob3497312476657191470.jar tmpDir=null</span><br><span class="line">19/12/31 12:51:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">19/12/31 12:51:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">19/12/31 12:51:08 INFO mapred.FileInputFormat: Total input paths to process : 2</span><br><span class="line">19/12/31 12:51:08 INFO mapreduce.JobSubmitter: number of splits:3</span><br><span class="line">19/12/31 12:51:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1577764863539_0010</span><br><span class="line">19/12/31 12:51:08 INFO impl.YarnClientImpl: Submitted application application_1577764863539_0010</span><br><span class="line">19/12/31 12:51:08 INFO mapreduce.Job: The url to track the job: http://DESKTOP-3EELJOI:8088/proxy/application_1577764863539_0010/</span><br><span class="line">19/12/31 12:51:08 INFO mapreduce.Job: Running job: job_1577764863539_0010</span><br><span class="line">19/12/31 12:51:19 INFO mapreduce.Job: Job job_1577764863539_0010 running in uber mode : false</span><br><span class="line">19/12/31 12:51:19 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/12/31 12:51:33 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/12/31 12:51:42 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">19/12/31 12:51:42 INFO mapreduce.Job: Job job_1577764863539_0010 completed successfully</span><br><span class="line">19/12/31 12:51:42 INFO mapreduce.Job: Counters: 49</span><br><span class="line">        File System Counters</span><br><span class="line">                FILE: Number of bytes read=1303853</span><br><span class="line">                FILE: Number of bytes written=3115959</span><br><span class="line">                FILE: Number of read operations=0</span><br><span class="line">                FILE: Number of large read operations=0</span><br><span class="line">                FILE: Number of write operations=0</span><br><span class="line">                HDFS: Number of bytes read=1271710</span><br><span class="line">                HDFS: Number of bytes written=2585774</span><br><span class="line">                HDFS: Number of read operations=12</span><br><span class="line">                HDFS: Number of large read operations=0</span><br><span class="line">                HDFS: Number of write operations=2</span><br><span class="line">        Job Counters</span><br><span class="line">                Launched map tasks=3</span><br><span class="line">                Launched reduce tasks=1</span><br><span class="line">                Data-local map tasks=3</span><br><span class="line">                Total time spent by all maps in occupied slots (ms)=36326</span><br><span class="line">                Total time spent by all reduces in occupied slots (ms)=6563</span><br><span class="line">                Total time spent by all map tasks (ms)=36326</span><br><span class="line">                Total time spent by all reduce tasks (ms)=6563</span><br><span class="line">                Total vcore-milliseconds taken by all map tasks=36326</span><br><span class="line">                Total vcore-milliseconds taken by all reduce tasks=6563</span><br><span class="line">                Total megabyte-milliseconds taken by all map tasks=37197824</span><br><span class="line">                Total megabyte-milliseconds taken by all reduce tasks=6720512</span><br><span class="line">        Map-Reduce Framework</span><br><span class="line">                Map input records=36535</span><br><span class="line">                Map output records=36535</span><br><span class="line">                Map output bytes=1230777</span><br><span class="line">                Map output materialized bytes=1303865</span><br><span class="line">                Input split bytes=302</span><br><span class="line">                Combine input records=0</span><br><span class="line">                Combine output records=0</span><br><span class="line">                Reduce input groups=3998</span><br><span class="line">                Reduce shuffle bytes=1303865</span><br><span class="line">                Reduce input records=36535</span><br><span class="line">                Reduce output records=32508</span><br><span class="line">                Spilled Records=73070</span><br><span class="line">                Shuffled Maps =3</span><br><span class="line">                Failed Shuffles=0</span><br><span class="line">                Merged Map outputs=3</span><br><span class="line">                GC time elapsed (ms)=498</span><br><span class="line">                CPU time spent (ms)=6307</span><br><span class="line">                Physical memory (bytes) snapshot=1049423872</span><br><span class="line">                Virtual memory (bytes) snapshot=1223659520</span><br><span class="line">                Total committed heap usage (bytes)=739246080</span><br><span class="line">        Shuffle Errors</span><br><span class="line">                BAD_ID=0</span><br><span class="line">                CONNECTION=0</span><br><span class="line">                IO_ERROR=0</span><br><span class="line">                WRONG_LENGTH=0</span><br><span class="line">                WRONG_MAP=0</span><br><span class="line">                WRONG_REDUCE=0</span><br><span class="line">        File Input Format Counters</span><br><span class="line">                Bytes Read=1271408</span><br><span class="line">        File Output Format Counters</span><br><span class="line">                Bytes Written=2585774</span><br><span class="line">19/12/31 12:51:42 INFO streaming.StreamJob: Output directory: /user/output</span><br></pre></td></tr></table></figure>

<p>这样输出就在hdfs上了，-cat查看中文乱码，可以下到本地</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -get /user/output/part-<span class="number">00000</span> ./output</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/mogician-alt/PicBed/dev/20191231125605.png" alt=""></p>
<h3 id="高级API方式-mrjob"><a href="#高级API方式-mrjob" class="headerlink" title="高级API方式(mrjob)"></a>高级API方式(mrjob)</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhdfs -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</span><br><span class="line">pip3 install mrjob -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</span><br><span class="line"><span class="keyword">from</span> mrjob.protocol <span class="keyword">import</span> RawProtocol, ReprProtocol</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Join</span><span class="params">(MRJob)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># mrjob allows you to specify input/intermediate/output serialization</span></span><br><span class="line">    <span class="comment"># default output protocol is JSON; here we set it to text</span></span><br><span class="line">    OUTPUT_PROTOCOL = RawProtocol</span><br><span class="line"></span><br><span class="line">    <span class="comment"># def mapper_init(self):</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, key, line)</span>:</span></span><br><span class="line">        <span class="comment"># note that the key is an object (a list in this case)</span></span><br><span class="line">        <span class="comment"># that mrjob will serialize as JSON text</span></span><br><span class="line">        data = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">yield</span> (data[<span class="number">0</span>], data[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combiner</span><span class="params">(self, key, values)</span>:</span></span><br><span class="line">        <span class="comment"># the combiner must be separate from the reducer because the input</span></span><br><span class="line">        <span class="comment"># and output must both be JSON</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            input_file = os.environ[<span class="string">'mapreduce_map_input_file'</span>]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            input_file = os.environ[<span class="string">'map_input_file'</span>]</span><br><span class="line">        input_file = os.path.basename(input_file).split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        self.tag = <span class="string">'sc'</span> <span class="keyword">if</span> input_file == <span class="string">'student_course'</span> <span class="keyword">else</span> <span class="string">'s'</span></span><br><span class="line">        <span class="keyword">yield</span> (key, [self.tag] + list(values))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, key, values)</span>:</span></span><br><span class="line">        <span class="comment"># the final output is encoded as text</span></span><br><span class="line">        scs = []</span><br><span class="line">        ss = []</span><br><span class="line">        <span class="keyword">for</span> val <span class="keyword">in</span> values:</span><br><span class="line">            <span class="keyword">if</span> val[<span class="number">0</span>] == <span class="string">'s'</span>:</span><br><span class="line">                ss += [val[<span class="number">1</span>]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                scs += [val[<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> ss:</span><br><span class="line">            <span class="keyword">for</span> sc <span class="keyword">in</span> scs:</span><br><span class="line">                <span class="keyword">yield</span> (key, <span class="string">'\t'</span>.join(s + sc))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># sets up a runner, based on command line options</span></span><br><span class="line">    Join.run()</span><br></pre></td></tr></table></figure>

<p>combiner应该也可以不写</p>
<p><img src="https://raw.githubusercontent.com/mogician-alt/PicBed/dev/20191231152313.png" alt=""></p>
<h2 id="JAVA版"><a href="#JAVA版" class="headerlink" title="JAVA版"></a>JAVA版</h2><p>一样是写mapper、reducer，还有Runner，代码就比较复杂了</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> joiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.omg.PortableInterceptor.SYSTEM_EXCEPTION;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String STUDENT_XLS = <span class="string">"student.xls"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String STUDENT_COURSE_XLS = <span class="string">"student_course.xls"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String STUDENT_FLAG = <span class="string">"student"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String STUDENT_COURSE_FLAG = <span class="string">"student_course"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FileSplit fileSplit;</span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> Text outValue = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">        String filePath = fileSplit.getPath().toString();</span><br><span class="line">        <span class="keyword">if</span> (filePath.contains(STUDENT_XLS))</span><br><span class="line">            outValue.set(STUDENT_FLAG + <span class="string">"\t"</span> + value);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (filePath.contains(STUDENT_COURSE_XLS))</span><br><span class="line">            outValue.set(STUDENT_COURSE_FLAG + <span class="string">"\t"</span> + value);</span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> joiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String STUDENT_FLAG = <span class="string">"student"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String STUDENT_COURSE_FLAG = <span class="string">"student_course"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String fileFlag = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> String stuName = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> List&lt;String&gt; stuClassNames;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> Text outValue = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String,List&lt;String&gt;&gt; sMap = <span class="keyword">new</span> HashMap&lt;String, List&lt;String&gt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String,List&lt;String&gt;&gt; scMap = <span class="keyword">new</span> IdentityHashMap&lt;String, List&lt;String&gt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String,List&lt;String&gt;&gt; finalMap = <span class="keyword">new</span> IdentityHashMap&lt;String, List&lt;String&gt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        stuClassNames = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        sMap.clear();</span><br><span class="line">        scMap.clear();</span><br><span class="line">        finalMap.clear();</span><br><span class="line">        <span class="keyword">for</span> (Text val : values) &#123;</span><br><span class="line">            String[] fields = StringUtils.split(val.toString(),<span class="string">"\t"</span>);</span><br><span class="line">            fileFlag = fields[<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">if</span> (fileFlag.equals(STUDENT_FLAG)) &#123;</span><br><span class="line">                List&lt;String&gt; line = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>;i&lt;fields.length;i++)</span><br><span class="line">                    line.add(fields[i]);</span><br><span class="line">                sMap.put(key.toString(),line);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (fileFlag.equals(STUDENT_COURSE_FLAG)) &#123;</span><br><span class="line">                List&lt;String&gt; line = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>;i&lt;fields.length;i++)</span><br><span class="line">                    line.add(fields[i]);</span><br><span class="line">                scMap.put(key.toString(),line);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String k :scMap.keySet()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (sMap.containsKey(k)) &#123;</span><br><span class="line">                List&lt;String&gt; strings = scMap.get(k);</span><br><span class="line">                strings.addAll(sMap.get(k));</span><br><span class="line">                finalMap.put(k,strings);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (String k:finalMap.keySet()) &#123;</span><br><span class="line">            List&lt;String&gt; strings = finalMap.get(k);</span><br><span class="line">            StringBuilder stringBuilder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">            <span class="keyword">for</span> (String string:strings)</span><br><span class="line">                stringBuilder.append(<span class="string">"\t"</span>).append(string);</span><br><span class="line">            outKey.set(k);</span><br><span class="line">            outValue.set(stringBuilder.toString());</span><br><span class="line">            context.write(outKey, outValue);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> joiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"><span class="keyword">import</span> tool.FileUtil;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.TreeMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinRunner</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> res = ToolRunner.run(<span class="keyword">new</span> Configuration(), <span class="keyword">new</span> JoinRunner(), <span class="keyword">new</span> String[]&#123;<span class="string">"input"</span>,<span class="string">"output"</span>&#125;);</span><br><span class="line">        System.exit(res);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        FileUtil.deleteDir(<span class="string">"output"</span>);</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf, <span class="string">"Join"</span>);</span><br><span class="line">        job.setJarByClass(JoinRunner.class);</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(JoinMapper.class);</span><br><span class="line">        job.setReducerClass(JoinReducer.class);</span><br><span class="line">        job.setInputFormatClass(ExcelInputFormat.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        <p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  </body>
</html>

